# -*- coding: utf-8 -*-
"""Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z4eEcjJROPJZjEUiEJkKUenXV7cqbr-T

# **Recommendation System Project: [(Indonesia Tourism Destination)](https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination)**
- **Nama:** Indira Aline
- **Email:** indiradira63@gmail.com
- **ID Dicoding:** indira_kbs

# **Import Semua Packages/Library yang Digunakan**
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import os

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import EarlyStopping
from pathlib import Path

# Mount Google Drive (Colab)
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

"""# **Data Loading**"""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

# Setup Kaggle API authentication
os.makedirs('/root/.kaggle', exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!ls -l /root/.kaggle/

!kaggle datasets download -d aprabowo/indonesia-tourism-destination

import zipfile,os,shutil

fileZip = "indonesia-tourism-destination.zip"
extracZip = zipfile.ZipFile(fileZip, 'r')
extracZip.extractall("dataset")

os.listdir("/content/dataset")

"""Dataset yang digunakan yaitu [Indonesia Tourism Destination](https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination) dengan `tourism_with_id.csv` dan `tourism_rating.csv` sebagai dataset.

# **Data Understanding**

Menampilkan dataset yaitu `tourism_with_id.csv` dan `tourism_rating.csv` menggunakan library pandas dengan format .csv menjadi dataframe.
"""

places = pd.read_csv('dataset/tourism_with_id.csv')
ratings = pd.read_csv('dataset/tourism_rating.csv')

print('Jumlah places: ', len(places.Place_Id.unique()))
print('Jumlah ratings: ', len(ratings.Place_Ratings))

"""## Exploratory Data Analysis

### Univariate

Tahap ini akan dilakukan proses investigasi pertama pada data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

2.1. _Tourism Destinations Dataset (Places)_

Meninjau informasi variabel dari _places dataset_ berupa jumlah kolom, nama kolom, jumlah data per kolom dan tipe data.
"""

places.info()

places.head()

places.describe()

"""2.2. _Ratings Dataset_

Memeriksa informasi variabel dari _ratings dataset_ berupa jumlah kolom, nama kolom, jumlah data per kolom dan tipe data.
"""

ratings.info()

ratings.head()

ratings.describe()

"""# **Data Preparation**

Di sini, data preparation merupakan proses transformasi data menjadi bentuk yang dapat diterima oleh model machine learning nantinya. Proses data preparation yang akan dilakukan, yaitu membersihkan data _missing value_, dan melakukan peninjauan data duplikat.

Menghapus Kolom yang Tidak Diperlukan

Data yang diperlukan hanya pada kolom `Place_Id`, `Place_Name`, dan `Category`, sisanya akan dihapus.
"""

places = places.drop(['Description', 'City', 'Price', 'Rating', 'Time_Minutes', 'Coordinate', 'Lat', 'Long', 'Unnamed: 11', 'Unnamed: 12'], axis=1)

"""Pengecekan Missing Value"""

places.isnull().sum()

ratings.isnull().sum()

"""Pengecekan Data Duplikat"""

print(f'Jumlah data places yang duplikat: {places.duplicated().sum()}')
print(f'Jumlah data rating yang duplikat: {ratings.duplicated().sum()}')

"""Melakukan penghapusan duplikat"""

ratings.drop_duplicates(inplace = True)

"""# **Modelling**

Proses pemodelan sistem rekomendasi ini dirancang untuk menghasilkan model yang dapat memberikan saran destinasi wisata terbaik sesuai dengan preferensi pengguna, dengan memanfaatkan teknik _content-based filtering_ dan _collaborative filtering_ berdasarkan penilaian yang diberikan

## _Content-based_

Teknik ini berfungsi menyarankan item mirip dengan riwayat preferensi pengguna. Prosesnya melibatkan pembuatan profil minat berdasarkan penilaian pengguna terhadap konten tertentu. Dengan memprioritaskan kemiripan fitur item, sistem berusaha menyajikan rekomendasi yang sesuai. Akurasinya bergantung pada kuantitas dan kualitas data yang diinput pengguna.

_TF-IDF Vectorizer_ berperan dalam menentukan atribut-atribut signifikan setiap klasifikasi tempat wisata. Menggunakan _library scikit-learn_, nilai-nilai tersebut diubah menjadi bentuk vektor melalui operasi _fit_transform_ dan _transform_, disertai proses pembagian data yang terintegrasi.
"""

tf = TfidfVectorizer()

tf.fit(places['Category'])

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(places['Category'])
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=places.Place_Name
).sample(10, axis=0)

"""### _Cosine Similarity_

Melakukan perhitungan derajat kesamaan atau _similatiry degree_ antar nama tempat wisata dengan teknik _cosine similarity_ menggunakan _library scikit-learn_.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(
    cosine_sim, index=places.Place_Name, columns=places.Place_Name)
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(10, axis=0)

"""### _Recommendation Testing_

Mengimplementasikan fungsi _place_recommendations_ guna menghasilkan rekomendasi tempat wisata berdasarkan tingkat kesamaan kategorinya.
"""

def place_recommendations(place_name, similarity_data=cosine_sim_df, items=places[['Place_Name', 'Category']], k=5):
    index = similarity_data.loc[:,place_name].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(place_name, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

place_name = 'Monumen Nasional'
places[places.Place_Name.eq(place_name)]

place_recommendations(place_name)

"""Output yang dihasilkan membuktikan efektivitas sistem dalam memberikan rekomendasi berdasarkan kesamaan kategori, seperti tampak pada contoh 'Monumen Nasional' yang merekomendasikan tempat-tempat budaya lainnya.

## _Collaborative Filtering_

Proses _encoding_ fitur `User_Id` pada dataset _ratings_ menjadi _array_.
"""

user_ids = ratings['User_Id'].unique().tolist()
print('list User_Id: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_Id : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User_Id: ', user_encoded_to_user)

"""Proses _encoding_ fitur `Place_Id` pada dataset _ratings_ menjadi _array_."""

place_ids = ratings['Place_Id'].unique().tolist()
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}

"""Meninjau jumlah user, jumlah tempat, penilaian minimal, dan penilaian maksimal."""

users_count = len(user_to_user_encoded)
place_count = len(place_encoded_to_place)

ratings['rating'] = ratings['Place_Ratings'].values.astype(np.float32)

min_rating = min(ratings['rating'])
max_rating = max(ratings['rating'])

print(f'Users Count: {users_count}')
print(f'Places Count: {place_count}')
print(f'Min rating: {min_rating}')
print(f'Max rating: {max_rating}')

"""## Split Data Latih dan Data Validasi

Acak _ratings dataset_
"""

ratings = ratings.sample(frac=1, random_state=42)
ratings

"""Membagi _dataset_ menjadi data latih (_train_) dan data uji (_test_), yaitu sebesar 20% data uji dan 80% data latih."""

x = ratings[['User_Id', 'Place_Id']].values
y = ratings['rating'].apply(lambda x: (
    x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development

_Layer embedding_ bertugas mengkonversi data numerik input menjadi bentuk vektor yang lebih kaya makna dan mudah diproses oleh algoritma _machine learning_.
"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, users_count, place_count, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.users_count = users_count
    self.place_count = place_count
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        users_count,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-8)
    )
    self.user_bias = layers.Embedding(users_count, 1)
    self.place_embedding = layers.Embedding(
        place_count,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-8)
    )
    self.place_bias = layers.Embedding(place_count, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    place_vector = self.place_embedding(inputs[:, 1])
    place_bias = self.place_bias(inputs[:, 1])

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x)

model = RecommenderNet(users_count, place_count, 50)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Menambahkan _callback EarlyStopping_ yang akan menghentikan _training_ jika tidak ada peningkatan selama 5 _epochs_."""

callbacks = EarlyStopping(
    min_delta=0.0001,
    patience=5,
    restore_best_weights=True,
)

"""Latih model

Apply encoding to both training and validation data and latih model using the encoded dat
"""

x_train_encoded = np.array([[user_to_user_encoded[user_id], place_to_place_encoded[place_id]]
                            for user_id, place_id in x_train])

x_val_encoded = np.array([[user_to_user_encoded[user_id], place_to_place_encoded[place_id]]
                          for user_id, place_id in x_val])

history = model.fit(
    x=x_train_encoded,
    y=y_train,
    batch_size=8,
    epochs=100,
    validation_data=(x_val_encoded, y_val),
    callbacks=[callbacks]
)

"""Visualisasi grafik data _training_ dan _testing_ untuk masing-masing metrik _Root Mean Square Error_ dan _loss function_."""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model error')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Uji Rekomendasi

Melakukan percobaan terhadap rekomendasi yang diberikan, termasuk kasus di mana pengguna belum memberikan penilaian pada tempat tertentu (`place_not_rated`).
"""

place_df = places
ratings_df = ratings

user_id = ratings_df.User_Id.sample(1).iloc[0]
place_rated = ratings_df[ratings_df.User_Id == user_id]

place_not_rated = place_df[~place_df['Place_Id'].isin(
    place_rated.Place_Id.values)]['Place_Id']
place_not_rated = list(
    set(place_not_rated).intersection(set(place_to_place_encoded.keys()))
)

place_not_rated = [
    [place_to_place_encoded.get(x)] for x in place_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_rated), place_not_rated)
)

"""Melakukan pengujian prediksi hasil rekomendasi tempat berdasarkan nama tempat dan kategori."""

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_rated[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('=====' * 8)
print('Place with high ratings from user')
print('-----' * 8)

top_place_user = (
    place_rated.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_df[place_df['Place_Id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.Place_Name + ':', row.Category)

print('-----' * 8)
print('Top 10 place recommendation')
print('-----' * 8)

recommended_place = place_df[place_df['Place_Id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.Place_Name + ':', row.Category)

"""Implementasi sistem yang diamati melibatkan seleksi random terhadap pengguna, kemudian menampilkan rekomendasi berupa lokasi-lokasi dengan nilai rating tertinggi dari user tersebut.

*   Curug Cilengkrang: Cagar Alam
*   Gunung Lalakon: Cagar Alam
*   Taman Hutan Raya Ir. H. Juanda: Cagar Alam
*   Grand Maerakaca: Taman Hiburan
*   Food Junction Grand Pakuwon: Pusat Perbelanjaan

Proses berikutnya menampilkan daftar 10 tempat rekomendasi yang disesuaikan dengan kategori dari pengguna acak tersebut. Nampak bahwa sistem memberikan beberapa rekomendasi dengan kategori identik, seperti:

*   Taman Vanda: Taman Hiburan
*   Taman Sejarah Bandung: Budaya
*   Puspa Iptek Sundial: Taman Hiburan
*   Museum Pendidikan Nasional: Budaya
*   Water Park Bandung Indah: Taman Hiburan
*   Curug Anom: Cagar Alam
*   Taman Film: Budaya
*   Museum Nike Ardilla: Budaya
*   Sanghyang Heuleut: Cagar Alam
*   Masjid Al-Imtizaj: Tempat Ibadah

# **Kesimpulan**

Dengan demikian, terbukti bahwa sistem mampu memberikan rekomendasi efektif melalui kedua pendekatan. _Collaborative Filtering_ memerlukan data rating pengguna, sementara _content-based filtering_ bekerja tanpa data penilaian karena menggunakan karakteristik tempat (kategori) sebagai dasar rekomendasi.
"""