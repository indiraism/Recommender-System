# -*- coding: utf-8 -*-
"""Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z4eEcjJROPJZjEUiEJkKUenXV7cqbr-T

# **Recommendation System Project: [(The Movie)](https://www.kaggle.com/datasets/parasharmanas/movie-recommendation-system)**
- **Nama:** Indira Aline
- **Email:** indiradira63@gmail.com
- **ID Dicoding:** indira_kbs

## Movie Recommendation

In this project, we will do a movies recommendation system. Today, the products launch very fast, there are a large number of new products every day. If we have recommendation system, users do not need to spend too much time to pick the one they interested. Which is conducive to increasing users stickiness.

Since we aleardy did a movies recommendation in HW3, we will do it in a different way. In the HW3, we use similarity measures for recommendation, which requires other user rate the movie first. In my project, we will cluster the users dan predict the movies rate by its genres and years. Using this method. We do not need the user rate the movies first, which is more suitable in real world.

# **Import Semua Packages/Library yang Digunakan**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
import re
import random
import os

# %matplotlib inline

from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans

# Mount Google Drive (Colab)
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

"""# **Data Preparation**

## Data Loading
"""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

# Setup Kaggle API authentication
os.makedirs('/root/.kaggle', exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!ls -l /root/.kaggle/

!kaggle datasets download -d parasharmanas/movie-recommendation-system

import zipfile,os,shutil

fileZip = "movie-recommendation-system.zip"
extracZip = zipfile.ZipFile(fileZip, 'r')
extracZip.extractall("dataset")

os.listdir("/content/dataset")

movies = pd.read_csv('dataset/movies.csv')
ratings = pd.read_csv('dataset/ratings.csv')
print(movies.info())
print(movies.head())

print(ratings.info())
print(ratings.head())

"""We can see that those movies contain tittle and genre, but in string type, we will do data cleaning and modification. Ignore the tittle, create new column 'years' and change the genres to one-hot coding.

## Exploratory Data Analysis
"""

genres = set()
genres_list = []
genres_count = dict()

for i in movies['genres']:
    gen = i.split('|')
    genres_list.append(gen)
    genres = genres.union(set(gen))
    for j in gen:
        if j not in genres_count:
            genres_count[j] = 1
        else:
            genres_count[j] += 1

print(f'There are {len(genres)} kinds of genres, they are {genres}')

# Convert number to percentage
movies_num = len(movies)
for k, v in genres_count.items():
    genres_count[k] = v/movies_num

plt.bar(*zip(*genres_count.items()))
plt.xticks(rotation=90)
plt.show()

"""Extract the years of movies from the title. If the movies do not contain the year or the year is out of range, we will set it to 'unknown'. The movies are from 1870s to 2010s, a long time span, and the year of movie itself does not affect people like it or not; we will convert it to 'new_movies_ratio', 'old_movies_ratio', which is new_movies/total_movies they watched, the sum of them will be one. Whenever the year of movie is after 2000 will be consider as new movies."""

years = []
years_in_decade = {'unknown': 0}
nonyear = []
for i in movies['title']:
    year_str = re.findall(r'\(\s*\+?(-?\d+)\s*\)', i)
    if year_str:
        year_int = int(year_str[-1])
    else:
        nonyear.append(i)
        year_int = 0
    if year_int < 1800 or year_int > 2024:
        year_int = 0
    years.append(year_int)

deca = year_int // 10 * 10
if deca == 0:
    years_in_decade['unknown'] += 1
elif str(deca) not in years_in_decade:
    years_in_decade[str(deca)] = 1
else:
    years_in_decade[str(deca)] += 1

cleaned_movies = pd.DataFrame(columns=['movieId', 'year', 'genres'])
cleaned_movies['movieId'] = movies['movieId']
cleaned_movies['year'] = years
cleaned_movies['genres'] = genres_list

print(cleaned_movies.head())

years_in_decade = {}

for year in years:
    if year < 1800 or year > 2024:
        decade = 'unknown'
    else:
        decade = str((year // 10) * 10)

    if decade not in years_in_decade:
        years_in_decade[decade] = 0
    years_in_decade[decade] += 1

# Hitung persentase
total_movies = len(years)
for decade in years_in_decade:
    years_in_decade[decade] = years_in_decade[decade] / total_movies

# Urutkan berdasarkan dekade
sorted_decades = sorted(years_in_decade.keys(), key=lambda x: float(x) if x != 'unknown' else -1)
sorted_percentages = [years_in_decade[decade] for decade in sorted_decades]

# Visualisasi
plt.figure(figsize=(12, 6))
plt.bar(sorted_decades, sorted_percentages)
plt.xlabel('Dekade')
plt.ylabel('Persentase Film')
plt.title('Distribusi Film Berdasarkan Dekade')
plt.xticks(rotation=45)
plt.show()

"""Next step, we will modify the 'ratings' dataset to 'user_preferences'. Calculate the user average rate, how many movies they watch, and which kind of genres they like

## Data Preprocessing
"""

genres_list = list(genres_count.keys())
columns = ['userId', 'number_of_movies', 'average_rate', 'new_movies_rate', 'old_movies_rate'] + genres_list
user_preferences = pd.DataFrame(columns=columns)
user_group = ratings.groupby('userId').agg(lambda x: list(x))

movie2index = pd.DataFrame(columns=['movieId', 'df_index'])
movie2index['df_index'] = cleaned_movies.index
movie2index['movieId'] = cleaned_movies['movieId']
movie2index.set_index('movieId', inplace=True)

print(user_group.head())

watched_movies = user_group['movieId'].str.len()
plt.boxplot(watched_movies)
plt.show()
print(f'The Min is {min(watched_movies)}, the Max is {max(watched_movies)}')

"""We can see that there is a extreme outlier and some small outliers in our dataset, and they are very extreme, and will affect our model performance. We need to get rid off them. We will remove what ever is higher than 1000"""

watched_movies_nooutlier = [x for x in watched_movies if x <= 1000]
plt.boxplot(watched_movies_nooutlier)
plt.show()

"""It looks much better, although it still have a lot of outliers, we will try using it first.

Before we using the cleaned_moveis, we want to conver the genres to one hot encoding; List is good for visualization, but is harder for computer to count.

## Data Cleaning
"""

v = cleaned_movies.genres.values
l = [len(x) for x in v.tolist()]
f, u = pd.factorize(np.concatenate(v))
n, m = len(v), u.size
i = np.arange(n).repeat(l)

dummies = pd.DataFrame(
    np.bincount(i * m + f, minlength=n * m).reshape(n, m),
    cleaned_movies.index, u
)
cleaned_movies['new_movies_rate'] = np.where(cleaned_movies['year'] >= 2000, 1, np.where(cleaned_movies['year'] < 2000, 0, 0))
cleaned_movies['old_movies_rate'] = np.where(cleaned_movies['year'] >= 2000, 0, np.where(cleaned_movies['year'] < 2000, 1, 1))
cleaned_movies = cleaned_movies.drop(['genres', 'year'], axis=1).join(dummies)

genres_cate = list(genres)

def movieid_to_index(movieid_list):
    return movie2index.loc[movieid_list]

def new_row(index):
    # group the movies by user view history
    row = user_group.loc[index]
    # get the movies info by movies id
    movies_info = cleaned_movies.loc[movieid_to_index(row['movieId'])['df_index'].tolist()]
    # Reset index to align with ratings
    movies_info = movies_info.reset_index(drop=True)
    movies_info = movies_info.drop(['movieId'], axis=1)
    # weight each row by the rating
    # Convert row['rating'] to a Series with the same index as movies_info
    rating_series = pd.Series(row['rating'], index=movies_info.index)
    movies_info = movies_info.mul(rating_series, axis=0)
    non_zero = movies_info.astype(bool).sum(axis=0)
    preferences = movies_info.sum()
    preferences = preferences.divide(non_zero)
    # add other information to the series
    preferences['average_rate'] = np.mean(row['rating'])
    preferences['number_of_movies'] = len(movies_info)
    # we can use either one of the nan filling method, 0 or mean
    preferences.fillna(0, inplace=True)
    preferences.fillna(preferences['average_rate'], inplace=True)
    preferences['userId'] = int(index)
    return preferences

sub_user = user_group[:8000]
i = 1
for index in sub_user.index:
    if i % 1000 == 0:
        print(f'finish ({i/80}%)')
    user_preferences = pd.concat([user_preferences, new_row(index).to_frame().T], ignore_index=True)
    i += 1
print(user_preferences.head())

user_preferences.to_csv('cleaned_data_sub.csv', index=False)

"""# **Modelling**

Now, we finish cleaning the data, and we are going the set up the model and test the performence. There are 160000+ of users, If using all of them it will cause the run time to be too long. Here we will only use a part of the data, which is 8000 users. But we will use all the movies for testing, it will also test the model how it perform during small training dataset.

We will compare three models hierarchical clustering, density-based clustering, and k-means
"""

user_preferences = pd.read_csv('cleaned_data_sub.csv')[:8000]

userid2index = pd.DataFrame(columns=['userId', 'df_index'])
userid2index['df_index'] = user_preferences.index
userid2index['userId'] = user_preferences['userId']
userid2index.set_index('userId', inplace=True)

movies_test = cleaned_movies.sample(frac=0.1, random_state=1)
user_test = user_preferences.sample(frac=0.2, random_state=1)
print(f'Number of movies for testing {len(movies_test)}')
print(f'Number of user for testing {len(user_test)}')

"""Because we have 20 genres, we will set our cluster to 20"""

train_df = user_preferences.drop(['userId', 'number_of_movies'], axis=1)
hierarchical_cluster = AgglomerativeClustering(n_clusters=20, metric='euclidean', linkage='ward')
hier_labels = hierarchical_cluster.fit_predict(train_df)

dens_cluster = DBSCAN(eps=1, min_samples=20)
dens_labels = dens_cluster.fit_predict(train_df)

kmeans = KMeans(n_clusters=20, random_state=0, n_init="auto")
kmeans_labels = kmeans.fit_predict(train_df)

print(hier_labels[:10])
print(dens_labels[:10])
print(kmeans_labels[:10])

"""# **Evaluation**

Because our model is designed to predict new movies, none of the movies in the test set have been rated by anyone. In this way, our model can be deployed on new movie recommendations. We evaluate the model in two ways:

1.   Collaborative-base: randomly choose some users and get their movies list, then compare the score they give with other similiar users. We will see how similiar with in the group.
2.   Content-base: set a threshold of good movies, and change the problem from rating prediction to binary classification, user will like it or not.

The first method is for old movies, or those a lot of users reted, and it had been use for many compnaies like eBay, Amazon, YouTube.

The second evaluation is situable for new movies, because we only care about whether we can recommend movvies that the audience is interested in, and we don't care about the scores users give to movies, and the movies they do not like.

Since only KMeans have the predict funtion, and DBSCAN, AgglomerativeClustering can only clustering, we will test mainly evaluatie the model in collaborative-base.
"""

def eva(labels, user_test):
    id_and_groups = pd.DataFrame(columns=['userId'])
    id_and_groups['userId'] = user_preferences['userId']
    id_and_groups['cluster'] = labels
    scoring_matrix = pd.merge(ratings[:130000], id_and_groups, on='userId')
    id_and_groups.set_index('userId', inplace=True)
    aver_sum = 0
    for en, i in enumerate(user_test['userId']):
        clus = labels[en]
        movieid = user_group.loc[i]['movieId'][0]
        m = scoring_matrix.loc[(scoring_matrix['cluster'] == int(clus)) & (scoring_matrix['movieId'] == movieid)]
        aver = m['rating'].mean() - scoring_matrix.loc[(scoring_matrix['userId'] == i) & (scoring_matrix['movieId'] == movieid)]['rating']
        aver_sum += np.abs(float(aver.sum()))
    return aver_sum/len(user_test)

print(f'The error of hiehierarchical clustering is {eva(hier_labels, user_test)}')
print(f'The error of DBSCAN is {eva(dens_labels, user_test)}')
print(f'The error of K-means is {eva(kmeans_labels, user_test)}')

"""## Tuning Model

Now we will tune our model find the best hyperparameters. We can change the n_clusters and metric.
"""

# we will grid search the DBSCAN and AgglomerativeClustering
eps = [0.1, 0.3, 0.6, 1, 1.3]
for i in eps:
    print(f'DBSCAN with eps {i} in error {eva(DBSCAN(eps=i, min_samples=20).fit_predict(train_df), user_test)}')

n_clu = [14, 16, 18, 20, 22]
for i in n_clu:
    err = eva(AgglomerativeClustering(n_clusters=i, linkage='ward').fit_predict(train_df), user_test)
    print(f'AgglomerativeClustering with n_cluster {i} in error {err}')

"""## Conclusion

This project is mainly focusing on cleaning and modifing the data, so we don't need to use complex model or do much model tuning. Our models give good performance, and DBSCAN have the best result.
"""